{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1e6G9G41BAeiZGj1GoxGu91097sPFsusg",
      "authorship_tag": "ABX9TyMWIAI3Mfrl7BN9OMpOQl7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdbrgo/PERCEUL/blob/umap-hdbscan/PERCEUL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "lIgbs-9bAhGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# preprocesing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin # used to define NumericSelector(), which is used in preprocessing\n",
        "\n",
        "# dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from umap import UMAP\n",
        "\n",
        "# cluster validation\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# clustering\n",
        "from sklearn.cluster import KMeans\n",
        "import hdbscan"
      ],
      "metadata": {
        "id": "Hmun0waZAq1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Hugging Face & Mount GDrive"
      ],
      "metadata": {
        "id": "Azdyovt6i3GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "# mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kAGfM795i2DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "hIaAGxgMAsYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_name = \"\"\n",
        "df = pd.read_csv(ds_name)"
      ],
      "metadata": {
        "id": "rY1VJeKzAxNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "XTN8u1XHAxuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=====================================================================\n",
        "# a custom and dynamic function for selecting numeric columns only.\n",
        "# will be used to make the pipeline\n",
        "class NumericSelector(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.numeric_cols_ = X.select_dtypes(include=[float, int]).columns\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_num = X[self.numeric_cols_]\n",
        "        return X_num\n",
        "#=====================================================================\n",
        "si = SimpleImputer(strategy='median')\n",
        "df_i = si.fit_transform(df)\n",
        "df_i = pd.DataFrame(df_i, columns=df.columns)\n",
        "\n",
        "ss = StandardScaler()\n",
        "df_i_ss = ss.fit_transform(df_i)\n",
        "df_p1 = pd.DataFrame(df_i_ss, columns=df.columns) # will undergo cluster exploration\n",
        "df_p2 = df_p1.copy() # will undergo final clustering"
      ],
      "metadata": {
        "id": "YbYB-pn6A5w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction 1"
      ],
      "metadata": {
        "id": "6xcgb0kjA-dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap_model = UMAP(n_components=2, random_state=42)\n",
        "df_p1 = umap_model.fit_transform(df_p1)"
      ],
      "metadata": {
        "id": "HdqWo-apt6To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Exploration\n"
      ],
      "metadata": {
        "id": "1sj9wcBpBDFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer1 = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
        "clusterer1.fit(df_p1)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.scatter(df_p1.iloc[:, 0], df_p1.iloc[:, 1], c=clusterer1.labels_, cmap='Paired')\n",
        "plt.title('HDBSCAN Clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSQdUZhBBFgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction 2"
      ],
      "metadata": {
        "id": "yrI7ZojQP8X_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGn6T4s7P_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Clustering"
      ],
      "metadata": {
        "id": "hKbh6GTwBF8p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTV8JaP8BP0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Interpretation"
      ],
      "metadata": {
        "id": "iDj7TmV_BJO2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iW8ss3DBS46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the PERCEUL Pipeline"
      ],
      "metadata": {
        "id": "mGuCHjjZ6KvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline for Cluster Exploration\n",
        "exploration_pipeline = Pipeline(steps = [\n",
        "    ('numeric_selector', NumericSelector()),\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('umap_model', UMAP(n_components=2, random_state=42))\n",
        "])\n",
        "\n",
        "# pipeline for Final Clustering (Production)\n",
        "core_pipeline = Pipeline(steps = [\n",
        "    ('numeric_selector', NumericSelector()),\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=2))\n",
        "])"
      ],
      "metadata": {
        "id": "wEzImQse6OAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}